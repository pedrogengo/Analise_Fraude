{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Aula 3 - Exercício - Pedro Gengo",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pedrogengo/Analise_Fraude/blob/master/Aula_3_Exerc%C3%ADcio_Pedro_Gengo_NN_TFIDF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OG5DT_dm6mk"
      },
      "source": [
        "# Notebook de referência \n",
        "\n",
        "Nome: Pedro Gabriel Gengo Lourenço"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Od7iUgHy5SSi"
      },
      "source": [
        "## Instruções\n",
        "\n",
        "- Treinar uma rede neural como classificador binário na tarefa de análise de sentimentos usando dataset IMDB.\n",
        "\n",
        "- Experimentar e reportar a acurácia usando 3 diferentes tipos de features como entrada:\n",
        "    1) Bag-of-words booleano\n",
        "    2) Bag-of-words com contagem das palavras (histograma das palavras)\n",
        "    3) TF-IDF\n",
        "\n",
        "Deve-se implementar o laço de treinamento e validação da rede neural.\n",
        "\n",
        "Neste exercício usaremos o IMDB com 20l exemplos para treino, 5k para desenvolvimento e 25k para teste."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXFdJz2KVeQw"
      },
      "source": [
        "## Preparando Dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHMi_Kq65fPM"
      },
      "source": [
        "Primeiro, fazemos download do dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wbnfzst5O3k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b76ad1b2-e2d5-4e19-b5fa-ac552aa0982d"
      },
      "source": [
        "!wget -nc http://files.fast.ai/data/aclImdb.tgz \n",
        "!tar -xzf aclImdb.tgz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-09-02 11:36:11--  http://files.fast.ai/data/aclImdb.tgz\n",
            "Resolving files.fast.ai (files.fast.ai)... 104.26.2.19, 104.26.3.19, 172.67.69.159, ...\n",
            "Connecting to files.fast.ai (files.fast.ai)|104.26.2.19|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://files.fast.ai/data/aclImdb.tgz [following]\n",
            "--2021-09-02 11:36:11--  https://files.fast.ai/data/aclImdb.tgz\n",
            "Connecting to files.fast.ai (files.fast.ai)|104.26.2.19|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 145982645 (139M) [application/x-gtar-compressed]\n",
            "Saving to: ‘aclImdb.tgz’\n",
            "\n",
            "aclImdb.tgz         100%[===================>] 139.22M  7.87MB/s    in 17s     \n",
            "\n",
            "2021-09-02 11:36:28 (8.24 MB/s) - ‘aclImdb.tgz’ saved [145982645/145982645]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Giyi5Rv_NIm"
      },
      "source": [
        "## Carregando o dataset\n",
        "\n",
        "Criaremos uma divisão de treino (80%) e validação (20%) artificialmente.\n",
        "\n",
        "Nota: Evitar de olhar ao máximo o dataset de teste para não ficar enviseado no que será testado. Em aplicações reais, o dataset de teste só estará disponível no futuro, ou seja, é quando o usuário começa a testar o seu produto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HIN_xLI_TuT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fae6bb59-1ed2-4e59-deb0-e3531b52f5b3"
      },
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "\n",
        "def load_texts(folder):\n",
        "    texts = []\n",
        "    for path in os.listdir(folder):\n",
        "        with open(os.path.join(folder, path)) as f:\n",
        "            texts.append(f.read())\n",
        "    return texts\n",
        "\n",
        "x_train_pos = load_texts('aclImdb/train/pos')\n",
        "x_train_neg = load_texts('aclImdb/train/neg')\n",
        "x_test_pos = load_texts('aclImdb/test/pos')\n",
        "x_test_neg = load_texts('aclImdb/test/neg')\n",
        "\n",
        "x_train = x_train_pos + x_train_neg\n",
        "x_test = x_test_pos + x_test_neg\n",
        "y_train = [True] * len(x_train_pos) + [False] * len(x_train_neg)\n",
        "y_test = [True] * len(x_test_pos) + [False] * len(x_test_neg)\n",
        "\n",
        "# Embaralhamos o treino para depois fazermos a divisão treino/valid.\n",
        "c = list(zip(x_train, y_train))\n",
        "random.shuffle(c)\n",
        "x_train, y_train = zip(*c)\n",
        "\n",
        "n_train = int(0.8 * len(x_train))\n",
        "\n",
        "x_valid = x_train[n_train:]\n",
        "y_valid = y_train[n_train:]\n",
        "x_train = x_train[:n_train]\n",
        "y_train = y_train[:n_train]\n",
        "\n",
        "print(len(x_train), 'amostras de treino.')\n",
        "print(len(x_valid), 'amostras de desenvolvimento.')\n",
        "print(len(x_test), 'amostras de teste.')\n",
        "\n",
        "print('3 primeiras amostras treino:')\n",
        "for x, y in zip(x_train[:3], y_train[:3]):\n",
        "    print(y, x[:100])\n",
        "\n",
        "print('3 últimas amostras treino:')\n",
        "for x, y in zip(x_train[-3:], y_train[-3:]):\n",
        "    print(y, x[:100])\n",
        "\n",
        "print('3 primeiras amostras validação:')\n",
        "for x, y in zip(x_valid[:3], y_test[:3]):\n",
        "    print(y, x[:100])\n",
        "\n",
        "print('3 últimas amostras validação:')\n",
        "for x, y in zip(x_valid[-3:], y_valid[-3:]):\n",
        "    print(y, x[:100])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20000 amostras de treino.\n",
            "5000 amostras de desenvolvimento.\n",
            "25000 amostras de teste.\n",
            "3 primeiras amostras treino:\n",
            "True Strangely, this version of OPEN YOUR EYES is more mature and more nuanced. Aided by hindsight, Crowe\n",
            "False It's a real challenge to make a movie about a baby being devoured by wild canines and the mother bei\n",
            "True This was a top-notch movie with a top-notch cast. Danny Glover, Tony Danza, Joseph Gordon-Levitt, an\n",
            "3 últimas amostras treino:\n",
            "False Most movies I can sit through easily, even if I do not particularly like the movie. I am the type of\n",
            "True Example of how a World War 2 documentary should be made,using first hand accounts from actual troops\n",
            "True I watched this last night on TV (HBO). I have to admit, that the tension in this movie was unsurpass\n",
            "3 primeiras amostras validação:\n",
            "True I purchased the DVD set on a recommendation from Amazon.com based on my other interests. They hit th\n",
            "True A very attractive and capable cast is lost in this deadly boring rehash of the slasher sub-genre. Th\n",
            "True i'm really getting old,,am in the midst of watching this 40 year old flick,and wonder what my grandc\n",
            "3 últimas amostras validação:\n",
            "True This is quite an unusual and unique little western, that is made mostly original due to its story th\n",
            "False An absolutely atrocious adaptation of the wonderful children's book. Crude and inappropriate humor, \n",
            "False A cheesy, compellingly awful (and NOT in a fun way) C Grade movie. Everything shouts 'amateur', from\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQJ1E_cKkXpT"
      },
      "source": [
        "## Processamento dos textos\n",
        "\n",
        "Nessa etapa, como descrito no enunciado do exercício, iremos realizar três tipos de vetorização:\n",
        "\n",
        "1. BoW booleano\n",
        "2. BoW com base na frequência\n",
        "3. TF-IDF\n",
        "\n",
        "É muito importante ressaltar a importância de aplicar o \"fit\" apenas no treino, ou seja, utilizar apenas o vocabulário do treino, e utilizar o que foi encontrado para o teste/validação.\n",
        "\n",
        "Para isso, usarei a estrutura de fit/transform bastante conhecida da biblioteca sklearn. Começarei, então, criando uma classe abstrata que será herdada na criação das outras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzlNA3zKmWBE"
      },
      "source": [
        "from abc import ABC, abstractmethod\n",
        " \n",
        "class Transformer(ABC):\n",
        " \n",
        "  @abstractmethod\n",
        "  def fit(self):\n",
        "      pass\n",
        "    \n",
        "  @abstractmethod\n",
        "  def transform(self):\n",
        "      pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpdDJMVUnSK0"
      },
      "source": [
        "## BoW (booleano e com frequência)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjnyDSRsnUW7"
      },
      "source": [
        "from collections import Counter\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "class BagOfWords(Transformer):\n",
        "  '''\n",
        "  Essa classe realiza a transformacao de uma lista de palavras\n",
        "  para uma lista de inteiros.\n",
        "\n",
        "  Attrs:\n",
        "    boolean(bool): Flag que define se o vetor gerado sera com base na\n",
        "      frequencia (contagem) ou com base na ocorrencia ou nao (bool) de\n",
        "      uma palavra do vocabulario.\n",
        "    max_size(int): Define o tamanho maximo do vocabulario. Caso usado com\n",
        "      use_unknown = True, o vocabulario tera o tamanho de max_size + 1.\n",
        "    stopwords(list): Define a lista de palavras que serao desconsideras na\n",
        "      geracao do vocabulario.\n",
        "    use_unknown(bool): Flag que define o uso ou nao de um elemento para\n",
        "      palavras que nao existem no vocabulario.\n",
        "  '''\n",
        "\n",
        "  def __init__(self, boolean=False, max_size=None, stopwords = [], use_unknown=False):\n",
        "    self.max_size = max_size\n",
        "    self.boolean = boolean\n",
        "    self.stopwords = stopwords\n",
        "    self.use_unknown = use_unknown\n",
        "\n",
        "  def _create_vocab(self, tokenized_texts):\n",
        "    '''\n",
        "    Cria o vocabulario que sera utilizado na transformacao do vetores\n",
        "    de palavras para vetores de inteiros.\n",
        "\n",
        "    Args:\n",
        "      tokenized_texts(list): Lista de textos ja tokenizados, ou seja,\n",
        "        uma lista onde cada elemento e um token.\n",
        "    \n",
        "    Return:\n",
        "      vocab(dict): Dicionario onde as chaves sao as palavras do vocabulario\n",
        "        e os valores representam o indice da palavra no vetor a ser gerado.\n",
        "    '''\n",
        "    counter = Counter()\n",
        "    for text in tokenized_texts:\n",
        "      counter.update(text)\n",
        "    for stop_word in self.stopwords:\n",
        "      if stop_word in counter.keys():\n",
        "        del counter[stop_word]\n",
        "    vocab = {element[0]: index for index, element in enumerate(counter.most_common(self.max_size))}\n",
        "    if self.use_unknown:\n",
        "      vocab['unknown'] = len(vocab)\n",
        "    return vocab\n",
        "  \n",
        "  def fit(self, texts):\n",
        "    '''\n",
        "    Metodo que cria os argumentos que serao utilizados nas\n",
        "    transformacoes posteriores. Esse metodo so deve ser utilizado \n",
        "    sobre o conjunto de treino.\n",
        "\n",
        "    Args:\n",
        "      texts(list): Lista de textos ja tokenizados, ou seja,\n",
        "        uma lista onde cada elemento e um token.\n",
        "    '''\n",
        "    vocab = self._create_vocab(texts)\n",
        "    self.vocabulary = vocab\n",
        "\n",
        "  def transform(self, texts):\n",
        "    '''\n",
        "    Realiza a transformacao de uma lista de tokens para uma\n",
        "    lista de inteiros com base no vocabulario criado na etapa\n",
        "    de fit.\n",
        "\n",
        "    Args:\n",
        "      texts(list): Lista de textos ja tokenizados, ou seja,\n",
        "        uma lista onde cada elemento e um token.\n",
        "    \n",
        "    Return:\n",
        "      bow_texts(torch.tensor): Array contendo os vetores de tokens\n",
        "        transformados para vetores de inteiros de tamanho fixo.\n",
        "    '''\n",
        "    transformed_texts = []\n",
        "    if self.use_unknown:\n",
        "      unknown = self.vocabulary.get('unknown')\n",
        "\n",
        "    for i, text in enumerate(texts):\n",
        "      bow_text = torch.zeros(len(self.vocabulary))\n",
        "      counter = Counter(text)\n",
        "\n",
        "      if self.use_unknown:\n",
        "        index = [self.vocabulary.get(key, unknown) for key in counter.keys() if key not in self.stopwords]\n",
        "      else:\n",
        "        index = [self.vocabulary[key] for key in counter.keys() if key in self.vocabulary.keys()]\n",
        "\n",
        "      if self.boolean:\n",
        "        bow_text[index] = 1\n",
        "      else:\n",
        "        values = [value for key, value in counter.items() if (self.use_unknown and key not in self.stopwords)\n",
        "                    or key in self.vocabulary.keys()]\n",
        "        bow_text[index] = torch.Tensor(values)\n",
        "\n",
        "      transformed_texts.append(bow_text)\n",
        "\n",
        "    return torch.vstack(transformed_texts).float()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTNcy1YhyylV"
      },
      "source": [
        "## Vocabulario deve ser: {'a': 0, 'texttinho': 1, 'testando': 2, 'b': 3, 'c': 4, 'unknown': 5}\n",
        "texts_test = [['text', 'texttinho', 'texttinho', 'testando'], ['a', 'b', 'a', 'a', 'c']]\n",
        "\n",
        "## Testando BoW com frequencia\n",
        "bow = BagOfWords(boolean=False, stopwords=['text'], use_unknown=True)\n",
        "bow.fit(texts_test)\n",
        "assert torch.all(bow.transform(texts_test) == torch.Tensor([[0., 2., 1., 0., 0., 0.], [3., 0., 0., 1., 1., 0.]]))\n",
        "\n",
        "## Testando BoW booleano\n",
        "bow = BagOfWords(boolean=True, stopwords=['text'], use_unknown=False)\n",
        "bow.fit(texts_test)\n",
        "assert torch.all(bow.transform(texts_test) == torch.Tensor([[0., 1., 1., 0., 0.], [1., 0., 0., 1., 1.]]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyp2AzOx1UQD"
      },
      "source": [
        "## TF-IDF\n",
        "\n",
        "$$\\text{TF-IDF}(t, d, C) = tf(t, d) * idf(t, C)$$\n",
        "\n",
        "Abrindo as funções definidas na equação principal:\n",
        "- $tf(t, d) = \\text{numero de vezes que o termo t aparece no documento d}$\n",
        "- $idf(t, C) = \\log{\\frac{C}{n_t}}$ \n",
        "\n",
        "Onde: \n",
        "\n",
        "- $\\text{t: token ou termo;}$\n",
        "- $\\text{d: documento(frase, enunciado, etc);}$\n",
        "- $\\text{C: Corpus (conjunto de documentos).}$\n",
        "- $n_t\\text{: numero de documentos onde o token t aparece.}$\n",
        "\n",
        "\n",
        " \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3Zv_fUG3mkb"
      },
      "source": [
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "class TfIdf(Transformer):\n",
        "  '''\n",
        "  Essa classe realiza a transformacao de uma lista de palavras\n",
        "  para uma lista de inteiros utilizando TFIDF.\n",
        "\n",
        "  Attrs:\n",
        "    max_size(int): Define o tamanho maximo do vocabulario. Caso usado com\n",
        "      use_unknown = True, o vocabulario tera o tamanho de max_size + 1.\n",
        "    stopwords(list): Define a lista de palavras que serao desconsideras na\n",
        "      geracao do vocabulario.\n",
        "  '''\n",
        "\n",
        "  def __init__(self, max_size=None, stopwords = []):\n",
        "    self.max_size = max_size\n",
        "    self.stopwords = stopwords\n",
        "\n",
        "  def _count_tokens_in_doc(self, tokenized_texts):\n",
        "    '''\n",
        "    Realiza a contagem de em quantos documentos uma mesma\n",
        "    palavra aparece, desconsiderando as stopwords.\n",
        "\n",
        "    Args:\n",
        "      tokenized_texts(list): Lista de textos ja tokenizados, ou seja,\n",
        "        uma lista onde cada elemento e um token.\n",
        "    \n",
        "    Return:\n",
        "      counter(collections.Counter): Objeto da classe Counter com todos\n",
        "        os elementos do conjunto de treino.\n",
        "    '''\n",
        "    counter = Counter()\n",
        "    for text in tokenized_texts:\n",
        "      counter.update(set(text))\n",
        "    for stop_word in self.stopwords:\n",
        "      if stop_word in counter.keys():\n",
        "        del counter[stop_word]\n",
        "    return counter\n",
        "  \n",
        "  def _create_idf(self, counter):\n",
        "    '''\n",
        "    Cria o vetor de idf para cada um dos tokens do conjunto de treino.\n",
        "\n",
        "    Args:\n",
        "      counter(collections.Counter): Objeto da classe Counter com todos\n",
        "        os elementos do conjunto de treino.\n",
        "    \n",
        "    Return:\n",
        "      idf(np.array): Array contendo o valor de idf para cada um dos tokens\n",
        "        do conjunto de treino.\n",
        "    '''\n",
        "    idf = [self.len_corpus/count for token, count in counter.most_common(self.max_size)]\n",
        "    return np.log(idf)\n",
        "\n",
        "  def _create_vocab(self, counter):\n",
        "    '''\n",
        "    Cria o vocabulario que sera utilizado na transformacao do vetores\n",
        "    de palavras para vetores de inteiros.\n",
        "\n",
        "    Args:\n",
        "      counter(collections.Counter): Objeto da classe Counter com todos\n",
        "        os elementos do conjunto de treino.\n",
        "    \n",
        "    Return:\n",
        "      vocab(dict): Dicionario onde as chaves sao as palavras do vocabulario\n",
        "        e os valores representam o indice da palavra no vetor a ser gerado.\n",
        "    '''\n",
        "    vocab = {element[0]: index for index, element in enumerate(counter.most_common(self.max_size))}\n",
        "    return vocab\n",
        "\n",
        "  def fit(self, texts):\n",
        "    '''\n",
        "    Metodo que cria os argumentos que serao utilizados nas\n",
        "    transformacoes posteriores. Esse metodo so deve ser utilizado \n",
        "    sobre o conjunto de treino.\n",
        "\n",
        "    Args:\n",
        "      texts(list): Lista de textos ja tokenizados, ou seja,\n",
        "        uma lista onde cada elemento e um token.\n",
        "    '''\n",
        "    self.len_corpus = len(texts)\n",
        "    counter = self._count_tokens_in_doc(texts)\n",
        "\n",
        "    self.vocabulary = self._create_vocab(counter)\n",
        "    self.idf = self._create_idf(counter)\n",
        "\n",
        "  def transform(self, texts):\n",
        "    '''\n",
        "    Realiza a transformacao de uma lista de tokens para uma\n",
        "    lista de inteiros com base no vocabulario criado na etapa\n",
        "    de fit.\n",
        "\n",
        "    Args:\n",
        "      texts(list): Lista de textos ja tokenizados, ou seja,\n",
        "        uma lista onde cada elemento e um token.\n",
        "    \n",
        "    Return:\n",
        "      tfidf_texts(torch.tensor): Array contendo os vetores de tokens\n",
        "        transformados para vetores de inteiros de tamanho fixo.\n",
        "    '''\n",
        "    transformed_texts = []\n",
        "\n",
        "    for i, text in enumerate(texts):\n",
        "      bow_text = torch.zeros(len(self.vocabulary))\n",
        "      counter = Counter(text)\n",
        "\n",
        "      index = []\n",
        "      values = []\n",
        "      for key, value in counter.items():\n",
        "        if key in self.vocabulary.keys():\n",
        "          index.append(self.vocabulary[key])\n",
        "          values.append(value)\n",
        "\n",
        "      bow_text[index] = torch.Tensor(values)\n",
        "\n",
        "      transformed_texts.append(bow_text * self.idf)\n",
        "\n",
        "    return torch.vstack(transformed_texts).float()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAKFMFu77hEM"
      },
      "source": [
        "## Vocabulario: {'t1': 0, 't2': 1, 't3': 2, 't4': 3}\n",
        "\n",
        "## Validando o TFIDF\n",
        "texts = [['t1', 't2', 't3', 't2', 't1'], ['t2', 't1'], ['t4', 't1']]\n",
        "tfidf = TfIdf()\n",
        "tfidf.fit(texts)\n",
        "assert torch.all(tfidf.transform(texts) - torch.Tensor([[0., 2 * np.log(3/2), np.log(3), 0.], [0., np.log(3/2), 0., 0.] , [0., 0., 0., np.log(3)]]) < 0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mOdOREM_g1I"
      },
      "source": [
        "## Tokenização\n",
        "\n",
        "Irei aplicar uma tokenização simples, onde irei remover a pontuação do texto e irei dividí-lo por palavras, ou seja, meus tokens serão as palavras que compõe a avaliação do filme."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUKtOiSXo-z1"
      },
      "source": [
        "from re import findall\n",
        "\n",
        "def tokenizer(texts):\n",
        "  tokenized_texts = []\n",
        "  for text in texts:\n",
        "    tokens = findall(r'\\w+|[^?\\-!.,:;\"\\'/><\\s]', text)\n",
        "    tokenized = [token.lower() for token in tokens]\n",
        "    tokenized_texts.append(tokenized)\n",
        "  return tokenized_texts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZbGyXLWpr8J"
      },
      "source": [
        "tokenized_x_train = tokenizer(x_train)\n",
        "tokenized_x_valid = tokenizer(x_valid)\n",
        "tokenized_x_test = tokenizer(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVqydRLn5Vl2"
      },
      "source": [
        "## Input para a rede neural\n",
        "\n",
        "No Pytorch, para treinarmos uma rede neural, é de extrema importância criarmos uma estrutura chamada Dataloader, a qual retorna, em cada iteração, um batch do tamanho definido. Para criarmos essa estrutura podemos definir uma outra classe, chamada Dataset, a qual temos que sobrescrever dois métodos: \\_\\_len\\_\\_ e \\_\\_getitem\\_\\_. Abaixo, podemos ver a definição do Dataset e a criação de uma função que cria os Dataloaders para utilizarmos no treino e teste. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lP9s66aRW7qM"
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class VectorizedDataset(Dataset):\n",
        "    def __init__(self, x, y):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.x)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.x[idx]\n",
        "        y = self.y[idx]\n",
        "        return x, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qoaOpkHSKQtZ"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def create_nn_input(tokenized_text_train, y_train,\n",
        "                    tokenized_text_valid, y_valid,\n",
        "                    tokenized_text_test, y_test,\n",
        "                    vectorizer, batch_size, shuffle):\n",
        "  \n",
        "  vectorizer.fit(tokenized_text_train)\n",
        "  vectorized_texts_train = vectorizer.transform(tokenized_text_train)\n",
        "  vectorized_texts_valid = vectorizer.transform(tokenized_text_valid)\n",
        "  vectorized_texts_test = vectorizer.transform(tokenized_text_test)\n",
        "\n",
        "  train_dataset = VectorizedDataset(vectorized_texts_train, torch.Tensor(y_train).reshape(-1,1))\n",
        "  valid_dataset = VectorizedDataset(vectorized_texts_valid, torch.Tensor(y_valid).reshape(-1,1))\n",
        "  test_dataset = VectorizedDataset(vectorized_texts_test, torch.Tensor(y_test).reshape(-1,1))\n",
        "\n",
        "  vectorized_texts_train = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle)\n",
        "  vectorized_texts_valid = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
        "  vectorized_texts_test = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "  return vectorized_texts_train, vectorized_texts_valid, vectorized_texts_test, vectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qn-p8WIK6Igl"
      },
      "source": [
        "## Definição da Rede Neural\n",
        "\n",
        "Na célula abaixo defino minha rede neural. Utilizarei 3 camadas escondidas com 128, 64 e 32 neurônios e, na camada de saída, teremos apenas uma saída e a aplicação de uma função sigmoide, que trará a probabilidade de ser positiva a avaliação (se > 0.5, é positiva, caso contrário, negativa)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-g4INOL1J63S"
      },
      "source": [
        "class MLP(torch.nn.Module):\n",
        "    def __init__(self, input_size, hidden_units=128):\n",
        "        super().__init__()\n",
        "        self.dense = torch.nn.Sequential(\n",
        "            torch.nn.Linear(input_size, 128),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(128, 64),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(64, 32),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(32, 1),\n",
        "            torch.nn.Sigmoid()\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.dense(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPbiUIrHZlun",
        "outputId": "6a42785f-ae7a-4693-e631-4a9da17a1a3b"
      },
      "source": [
        "if torch.cuda.is_available(): \n",
        "   dev = \"cuda:0\"\n",
        "   print(torch. cuda. get_device_name(dev))\n",
        "else: \n",
        "   dev = \"cpu\" \n",
        "print(dev)\n",
        "device = torch.device(dev)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tesla K80\n",
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4pcdOkoPF_w",
        "outputId": "cd01a9c8-a487-4fad-f13f-212ebe5af8b9"
      },
      "source": [
        "mlp = MLP(10000)\n",
        "mlp.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLP(\n",
              "  (dense): Sequential(\n",
              "    (0): Linear(in_features=10000, out_features=128, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
              "    (3): ReLU()\n",
              "    (4): Linear(in_features=64, out_features=32, bias=True)\n",
              "    (5): ReLU()\n",
              "    (6): Linear(in_features=32, out_features=1, bias=True)\n",
              "    (7): Sigmoid()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TMqzN6a63Oh"
      },
      "source": [
        "### Treino da rede neural\n",
        "\n",
        "Abaixo, temos a definição do loop de treino:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DWzhTJlZRyk"
      },
      "source": [
        "def train(model, train, valid, criterion, optimizer,\n",
        "          max_size, filename_save, batch_size=128, n_epochs=10):\n",
        "  \n",
        "  best_valid_loss = 10e9\n",
        "  best_epoch = 0\n",
        "\n",
        "  for i in range(n_epochs):\n",
        "    accumulated_loss = 0\n",
        "    model.train()\n",
        "    for x_train, y_train in train:\n",
        "      x_train = x_train.to(device)\n",
        "      y_train = y_train.to(device)\n",
        "      outputs = model(x_train)\n",
        "      batch_loss = criterion(outputs, y_train)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      batch_loss.backward()\n",
        "      optimizer.step()\n",
        "      accumulated_loss += batch_loss.item()\n",
        "\n",
        "    train_loss = accumulated_loss / len(train.dataset)\n",
        "    \n",
        "    # Laço de Validação, um a cada época.\n",
        "    accumulated_loss = 0\n",
        "    accumulated_accuracy = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for x_valid, y_valid in valid:\n",
        "            x_valid = x_valid.to(device)\n",
        "            y_valid = y_valid.to(device)\n",
        "\n",
        "            # predict da rede\n",
        "            outputs = model(x_valid)\n",
        "\n",
        "            # calcula a perda\n",
        "            batch_loss = criterion(outputs, y_valid)\n",
        "            preds = outputs > 0.5\n",
        "\n",
        "            # calcula a acurácia\n",
        "            batch_accuracy = (preds == y_valid).sum()\n",
        "            accumulated_loss += batch_loss\n",
        "            accumulated_accuracy += batch_accuracy\n",
        "\n",
        "    valid_loss = accumulated_loss / len(valid.dataset)\n",
        "    valid_acc = accumulated_accuracy / len(valid.dataset)\n",
        "    print(f'Época: {i:d}/{n_epochs - 1:d} Train Loss: {train_loss:.6f} Valid Loss: {valid_loss:.6f} Valid Acc: {valid_acc:.3f}')\n",
        "\n",
        "    # Salvando o melhor modelo de acordo com a loss de validação\n",
        "    if valid_loss < best_valid_loss:\n",
        "        torch.save(model.state_dict(), filename_save + '.pt')\n",
        "        best_valid_loss = valid_loss\n",
        "        best_epoch = i\n",
        "        print('best model')\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayXsGnyW8lmo"
      },
      "source": [
        "## Loss e otimizador:\n",
        "\n",
        "Na célula a seguir estou definindo a Loss a ser utilizada (binary cross entropy, já que temos apenas duas classes) e Stochastic gradient descent como otimizador. Manterei os parâmetros para todos os experimentos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Ynox3OWO0lu"
      },
      "source": [
        "learningRate = 0.01\n",
        "\n",
        "# Utilizaremos CrossEntropyLoss como função de perda\n",
        "criterion = torch.nn.BCELoss()\n",
        "\n",
        "# Gradiente descendente\n",
        "optimizer = torch.optim.SGD(mlp.parameters(), lr=learningRate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvPTgVXv_6xv"
      },
      "source": [
        "## Avaliação no conjunto de teste:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQ8xI9Df-qxi"
      },
      "source": [
        "def predict(model, state_dict, test):\n",
        "  accumulated_accuracy = 0\n",
        "  model.load_state_dict(torch.load(state_dict + '.pt'))\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "      for x_test, y_test in test:\n",
        "          x_test = x_test.to(device)\n",
        "          y_test = y_test.to(device)\n",
        "\n",
        "          # predict da rede\n",
        "          outputs = model(x_test)\n",
        "\n",
        "          # calcula a perda\n",
        "          batch_loss = criterion(outputs, y_test)\n",
        "          preds = outputs > 0.5\n",
        "\n",
        "          # calcula a acurácia\n",
        "          batch_accuracy = (preds == y_test).sum()\n",
        "          accumulated_accuracy += batch_accuracy\n",
        "\n",
        "  test_acc = accumulated_accuracy / len(test.dataset)\n",
        "  test_acc *= 100\n",
        "  print('*' * 40)\n",
        "  print(f'Acurácia de {test_acc:.3f} %')\n",
        "  print('*' * 40)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCd7irmn9N9r"
      },
      "source": [
        "## Experimentos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qY2s0vY99RMV"
      },
      "source": [
        "### Bow Booleano"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-C_3NNh9_Pq"
      },
      "source": [
        "max_size = 3000\n",
        "batch_size = 64\n",
        "n_epochs = 10\n",
        "learningRate = 0.1\n",
        "save_filename = 'bow_bool'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ckg6CA2L9oCt",
        "outputId": "d6e363d7-eaaa-4291-cc3d-1d47f9d546f3"
      },
      "source": [
        "mlp_bow_bool = MLP(max_size)\n",
        "mlp_bow_bool.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLP(\n",
              "  (dense): Sequential(\n",
              "    (0): Linear(in_features=3000, out_features=128, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
              "    (3): ReLU()\n",
              "    (4): Linear(in_features=64, out_features=32, bias=True)\n",
              "    (5): ReLU()\n",
              "    (6): Linear(in_features=32, out_features=1, bias=True)\n",
              "    (7): Sigmoid()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSKZ5sZu9Xwb"
      },
      "source": [
        "criterion = torch.nn.BCELoss()\n",
        "optimizer = torch.optim.SGD(mlp_bow_bool.parameters(), lr=learningRate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4_1T0bX9p-E",
        "outputId": "9c994542-7b29-4256-9cc1-22ce45a79388"
      },
      "source": [
        "vectorizer = BagOfWords(boolean=True, max_size = max_size)\n",
        "\n",
        "vectorized_train, vectorized_valid, vectorized_test, _ = create_nn_input(tokenized_x_train, y_train,\n",
        "                                                                         tokenized_x_valid, y_valid,\n",
        "                                                                         tokenized_x_test, y_test,\n",
        "                                                                         vectorizer, batch_size, True)\n",
        "\n",
        "_ = train(mlp_bow_bool, vectorized_train, vectorized_valid, criterion, optimizer,\n",
        "          max_size, save_filename, batch_size, n_epochs=n_epochs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Época: 0/9 Train Loss: 0.009692 Valid Loss: 0.009943 Valid Acc: 0.681\n",
            "best model\n",
            "Época: 1/9 Train Loss: 0.005779 Valid Loss: 0.005276 Valid Acc: 0.859\n",
            "best model\n",
            "Época: 2/9 Train Loss: 0.004612 Valid Loss: 0.004720 Valid Acc: 0.878\n",
            "best model\n",
            "Época: 3/9 Train Loss: 0.003998 Valid Loss: 0.004823 Valid Acc: 0.877\n",
            "Época: 4/9 Train Loss: 0.003636 Valid Loss: 0.005820 Valid Acc: 0.852\n",
            "Época: 5/9 Train Loss: 0.003218 Valid Loss: 0.004896 Valid Acc: 0.878\n",
            "Época: 6/9 Train Loss: 0.003038 Valid Loss: 0.007642 Valid Acc: 0.824\n",
            "Época: 7/9 Train Loss: 0.002545 Valid Loss: 0.005851 Valid Acc: 0.869\n",
            "Época: 8/9 Train Loss: 0.002161 Valid Loss: 0.006246 Valid Acc: 0.849\n",
            "Época: 9/9 Train Loss: 0.001317 Valid Loss: 0.006564 Valid Acc: 0.880\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2AwVpypAGd4",
        "outputId": "88844905-0f03-4e72-88b8-0f9fd4ff9299"
      },
      "source": [
        "predict(mlp_bow_bool, save_filename, vectorized_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "****************************************\n",
            "Acurácia de 87.428 %\n",
            "****************************************\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QzuMHMT9u02"
      },
      "source": [
        "### BoW frequência"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDp44vxp-FJE"
      },
      "source": [
        "max_size = 3000\n",
        "batch_size = 64\n",
        "n_epochs = 10\n",
        "learningRate = 0.1\n",
        "save_filename = 'bow_freq'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8eqq3FnW-FJF",
        "outputId": "c53c56f7-a338-472a-f48b-856383a93e8f"
      },
      "source": [
        "mlp_bow_freq = MLP(max_size)\n",
        "mlp_bow_freq.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLP(\n",
              "  (dense): Sequential(\n",
              "    (0): Linear(in_features=3000, out_features=128, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
              "    (3): ReLU()\n",
              "    (4): Linear(in_features=64, out_features=32, bias=True)\n",
              "    (5): ReLU()\n",
              "    (6): Linear(in_features=32, out_features=1, bias=True)\n",
              "    (7): Sigmoid()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ly6LhNC9-FJF"
      },
      "source": [
        "criterion = torch.nn.BCELoss()\n",
        "optimizer = torch.optim.SGD(mlp_bow_freq.parameters(), lr=learningRate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNayZFPs-FJG",
        "outputId": "758c6d04-34ec-4c71-a5f8-b81dd394dac4"
      },
      "source": [
        "vectorizer = BagOfWords(max_size=max_size)\n",
        "\n",
        "vectorized_train, vectorized_valid, vectorized_test, _ = create_nn_input(tokenized_x_train, y_train,\n",
        "                                                                         tokenized_x_valid, y_valid,\n",
        "                                                                         tokenized_x_test, y_test,\n",
        "                                                                         vectorizer, batch_size, True)\n",
        "\n",
        "_ = train(mlp_bow_freq, vectorized_train, vectorized_valid, criterion, optimizer,\n",
        "          max_size, save_filename, batch_size, n_epochs=n_epochs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Época: 0/9 Train Loss: 0.009816 Valid Loss: 0.009222 Valid Acc: 0.710\n",
            "best model\n",
            "Época: 1/9 Train Loss: 0.008397 Valid Loss: 0.008890 Valid Acc: 0.679\n",
            "best model\n",
            "Época: 2/9 Train Loss: 0.007475 Valid Loss: 0.007441 Valid Acc: 0.831\n",
            "best model\n",
            "Época: 3/9 Train Loss: 0.006785 Valid Loss: 0.007246 Valid Acc: 0.768\n",
            "best model\n",
            "Época: 4/9 Train Loss: 0.006150 Valid Loss: 0.006105 Valid Acc: 0.833\n",
            "best model\n",
            "Época: 5/9 Train Loss: 0.005869 Valid Loss: 0.008014 Valid Acc: 0.727\n",
            "Época: 6/9 Train Loss: 0.005582 Valid Loss: 0.005198 Valid Acc: 0.863\n",
            "best model\n",
            "Época: 7/9 Train Loss: 0.005580 Valid Loss: 0.006646 Valid Acc: 0.810\n",
            "Época: 8/9 Train Loss: 0.005123 Valid Loss: 0.005766 Valid Acc: 0.843\n",
            "Época: 9/9 Train Loss: 0.005127 Valid Loss: 0.007679 Valid Acc: 0.788\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aKC_uhYtAje3",
        "outputId": "40834ff3-7994-4514-9667-8fd9bdd7350f"
      },
      "source": [
        "predict(mlp_bow_freq, save_filename, vectorized_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "****************************************\n",
            "Acurácia de 85.860 %\n",
            "****************************************\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfyrRMue-JZh"
      },
      "source": [
        "### TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2hZ6AHP-MzH"
      },
      "source": [
        "max_size = 3000\n",
        "batch_size = 64\n",
        "n_epochs = 10\n",
        "learningRate = 0.01\n",
        "save_filename = 'tfidf'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kaV-SXEF-MzI",
        "outputId": "7cd110f4-33d1-47f1-fd01-5fa0302fc030"
      },
      "source": [
        "mlp_tfidf = MLP(max_size)\n",
        "mlp_tfidf.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLP(\n",
              "  (dense): Sequential(\n",
              "    (0): Linear(in_features=3000, out_features=128, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
              "    (3): ReLU()\n",
              "    (4): Linear(in_features=64, out_features=32, bias=True)\n",
              "    (5): ReLU()\n",
              "    (6): Linear(in_features=32, out_features=1, bias=True)\n",
              "    (7): Sigmoid()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T49fixlm-MzI"
      },
      "source": [
        "criterion = torch.nn.BCELoss()\n",
        "optimizer = torch.optim.SGD(mlp_tfidf.parameters(), lr=learningRate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_oR8jxQ-MzJ",
        "outputId": "54570645-9b12-40f5-c198-15e58f3a539b"
      },
      "source": [
        "vectorizer = TfIdf(max_size=max_size)\n",
        "\n",
        "vectorized_train, vectorized_valid, vectorized_test, _ = create_nn_input(tokenized_x_train, y_train,\n",
        "                                                                         tokenized_x_valid, y_valid,\n",
        "                                                                         tokenized_x_test, y_test,\n",
        "                                                                         vectorizer, batch_size, True)\n",
        "\n",
        "_ = train(mlp_tfidf, vectorized_train, vectorized_valid, criterion, optimizer,\n",
        "          max_size, save_filename, batch_size, n_epochs=n_epochs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Época: 0/9 Train Loss: 0.010820 Valid Loss: 0.010893 Valid Acc: 0.620\n",
            "best model\n",
            "Época: 1/9 Train Loss: 0.010690 Valid Loss: 0.010656 Valid Acc: 0.721\n",
            "best model\n",
            "Época: 2/9 Train Loss: 0.009883 Valid Loss: 0.008769 Valid Acc: 0.824\n",
            "best model\n",
            "Época: 3/9 Train Loss: 0.006628 Valid Loss: 0.005427 Valid Acc: 0.870\n",
            "best model\n",
            "Época: 4/9 Train Loss: 0.004588 Valid Loss: 0.004823 Valid Acc: 0.883\n",
            "best model\n",
            "Época: 5/9 Train Loss: 0.003893 Valid Loss: 0.005214 Valid Acc: 0.877\n",
            "Época: 6/9 Train Loss: 0.003495 Valid Loss: 0.004955 Valid Acc: 0.883\n",
            "Época: 7/9 Train Loss: 0.003219 Valid Loss: 0.006126 Valid Acc: 0.871\n",
            "Época: 8/9 Train Loss: 0.003010 Valid Loss: 0.005969 Valid Acc: 0.870\n",
            "Época: 9/9 Train Loss: 0.002820 Valid Loss: 0.005700 Valid Acc: 0.872\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1BnY0cV3w4P",
        "outputId": "4214d44d-b4b1-4a88-ad15-1d9bcdb17370"
      },
      "source": [
        "predict(mlp_tfidf, save_filename, vectorized_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "****************************************\n",
            "Acurácia de 88.080 %\n",
            "****************************************\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1in5iPTJCwVt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}